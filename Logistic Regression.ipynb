{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748341e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, matthews_corrcoef,balanced_accuracy_score, f1_score,recall_score,precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score, precision_recall_curve\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fe5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = 'Jeyaraman_2020_cleaned.csv'\n",
    "# file_path = 'Radjenovic_2013_cleaned.csv'\n",
    "# file_path = 'Smid_2020_cleaned.csv'\n",
    "# file_path = 'Hall_2012_cleaned.csv'\n",
    "\n",
    "df = pd.read_csv(file_path, delimiter=',')\n",
    "df = df.dropna(axis=0)\n",
    "df_sample = df.copy()\n",
    "df_sample = df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e62a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import random\n",
    "# from nltk.corpus import wordnet\n",
    "\n",
    "# def get_synonyms(word):\n",
    "#     synonyms = set()\n",
    "#     for syn in wordnet.synsets(word):\n",
    "#         for lemma in syn.lemmas():\n",
    "#             synonym = lemma.name().replace('_', ' ')\n",
    "#             synonyms.add(synonym)\n",
    "#     if word in synonyms:\n",
    "#         synonyms.remove(word)\n",
    "#     return list(synonyms)\n",
    "\n",
    "# def synonym_replacement(sentence, n):\n",
    "#     words = sentence.split()\n",
    "#     new_words = words.copy()\n",
    "#     random_word_list = list(set([word for word in words if word.isalpha()]))\n",
    "#     random.shuffle(random_word_list)\n",
    "#     num_replaced = 0\n",
    "#     for random_word in random_word_list:\n",
    "#         synonyms = get_synonyms(random_word)\n",
    "#         if len(synonyms) >= 1:\n",
    "#             synonym = random.choice(list(synonyms))\n",
    "#             new_words = [synonym if word == random_word else word for word in new_words]\n",
    "#             num_replaced += 1\n",
    "#         if num_replaced >= n: # only replace up to n words\n",
    "#             break\n",
    "\n",
    "#     sentence = ' '.join(new_words)\n",
    "#     return sentence\n",
    "\n",
    "# def augment_text(df, minority_class, augment_by=0.9):\n",
    "#     minority_df = df[df['label_included'] == minority_class]\n",
    "    \n",
    "#     n_minority = len(minority_df)\n",
    "#     n_augmentations = int(n_minority * augment_by)\n",
    "    \n",
    "#     augmented_texts = []\n",
    "#     for _ in range(n_augmentations):\n",
    "#         original_text = random.choice(minority_df['Corpus'].tolist())\n",
    "#         augmented_text = synonym_replacement(original_text, n=1) # You can adjust n for more replacements\n",
    "#         augmented_texts.append(augmented_text)\n",
    "    \n",
    "#     # Create a DataFrame for augmented minority samples\n",
    "#     augmented_df = pd.DataFrame(augmented_texts, columns=['Corpus'])\n",
    "#     augmented_df['label_included'] = minority_class\n",
    "    \n",
    "#     return augmented_df\n",
    "\n",
    "# # Assuming your minority class is identified, for example, as 1\n",
    "# df_augmented = augment_text(df, minority_class=1, augment_by=0.5)\n",
    "\n",
    "\n",
    "# df_sample = pd.concat([df, df_augmented], ignore_index=True)\n",
    "\n",
    "# # Shuffle the dataframe to mix original and augmented examples (optional)\n",
    "# df_sample = df_sample.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a5df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sample['Corpus']\n",
    "y= df_sample['label_included']\n",
    "# Encode labels to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b60278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df_augmented['label_included'].value_counts()\n",
    "print(class_counts)\n",
    "class_counts = df_sample['label_included'].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "X = df_sample['Corpus']\n",
    "y= df_sample['label_included']\n",
    "# Encode labels to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ba497",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],  # Regularization penalty\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
    "    'solver': ['liblinear', 'saga'],  # Algorithm to use in the optimization problem\n",
    "    'max_iter': [100, 200, 300]  # Maximum number of iterations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283e9855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote_enn = SMOTEENN()\n",
    "logistic_classifier = LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator=logistic_classifier, param_grid=param_grid, scoring='accuracy', cv=NUM_FOLDS, n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dad9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_evaluate_logistic_regression(X_train, y_train, X_test, y_test, param_grid, kfold):\n",
    "    # Data preprocessing\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(2, 3), max_df=0.7)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    X_train_tfidf_std = scaler.fit_transform(X_train_tfidf)\n",
    "    X_test_tfidf_std = scaler.transform(X_test_tfidf)\n",
    "\n",
    "\n",
    "\n",
    "    # Logistic Regression model training and evaluation\n",
    "    grid_search = GridSearchCV(estimator=LogisticRegression(), param_grid=param_grid, scoring='accuracy', cv=kfold, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train_tfidf_std, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_lr_classifier = grid_search.best_estimator_\n",
    "\n",
    "    best_lr_classifier.fit(X_train_tfidf_std, y_train)\n",
    "    y_pred = best_lr_classifier.predict(X_test_tfidf_std)\n",
    "\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "\n",
    "    return best_params, balanced_acc, mcc, f1, recall,precision, tfidf_vectorizer,best_lr_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside the loop, call the modified function:\n",
    "stratified_kfold = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Lists to store evaluation metrics\n",
    "balanced_acc_scores = []\n",
    "mcc_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores =[]\n",
    "\n",
    "\n",
    "\n",
    "for fold_idx, (train_index, test_index) in enumerate(stratified_kfold.split(X, y_encoded), 1):\n",
    "    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "    # Train and evaluate Logistic Regression model\n",
    "    best_params, balanced_acc_fold, mcc_fold, f1_fold, recall_fold, precision_fold,tfidf_vectorizer, best_lr_classifier = train_evaluate_logistic_regression(\n",
    "        X_train_fold, y_train_fold, X_test_fold, y_test_fold, param_grid, stratified_kfold\n",
    "    )\n",
    "\n",
    "    # Append scores to lists\n",
    "    balanced_acc_scores.append(balanced_acc_fold)\n",
    "    mcc_scores.append(mcc_fold)\n",
    "    f1_scores.append(f1_fold)\n",
    "    recall_scores.append(recall_fold)\n",
    "    precision_scores.append(precision_fold)\n",
    "    \n",
    "\n",
    "    # Print results for the fold\n",
    "    print(f\"Fold {fold_idx}:\")\n",
    "    print(f\"  Best Hyperparameters: {best_params}\")\n",
    "    print(f\"  Balanced Accuracy: {balanced_acc_fold:.2f}\")\n",
    "    print(f\"  F1-Score: {f1_fold:.2f}\")\n",
    "    print(f\"  Matthew's Correlation Coefficient: {mcc_fold:.2f}\")\n",
    "    print(f\"  Recall: {recall_fold:.2f}\")\n",
    "    print(f\"  Precision: {precision_fold:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c0182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Scores tfidf:\")\n",
    "print(f\"{np.mean(mcc_scores):.2f}\")\n",
    "print(f\"{np.mean(balanced_acc_scores):.2f}\")\n",
    "print(f\"{np.mean(f1_scores):.2f}\")\n",
    "print(f\"{np.mean(precision_scores):.2f}\")\n",
    "print(f\"{np.mean(recall_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5864878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ...other imports and model code as necessary...\n",
    "\n",
    "# Assuming 'texts' is a list of strings you want to explain\n",
    "for i, text in enumerate(texts):\n",
    "    # Create an explanation for the i-th text\n",
    "    exp = explainer.explain_instance(text, predict_proba, num_features=20, labels=[1], num_samples=50)\n",
    "    exp.show_in_notebook(text=True)\n",
    "    \n",
    "    # Save the explanation figure if needed\n",
    "    fig = exp.as_pyplot_figure()\n",
    "    fig.savefig(f'lime_explanation_{i}.png', bbox_inches='tight')  # Saves each explanation as a separate PNG file\n",
    "\n",
    "    # Convert the explanation to a list of tuples and then to a DataFrame\n",
    "    feature_names = exp.as_list(label=1)\n",
    "    df = pd.DataFrame(feature_names, columns=['Feature', 'Weight'])\n",
    "\n",
    "    # Display the DataFrame for this explanation\n",
    "    print(df)\n",
    "\n",
    "# Code to remove important features according to LIME\n",
    "# Extract only the feature names from the explanations\n",
    "important_features = [feature for feature, weight in exp.as_list(label=1)]\n",
    "\n",
    "# This assumes you have a way to map from feature names back to your feature matrix columns\n",
    "# Here, we mock this by assuming 'feature_mapping' is a dictionary that maps from feature names to column indexes\n",
    "# feature_mapping = {'feature_name': column_index, ...}\n",
    "important_indices = [feature_mapping[feature] for feature in important_features if feature in feature_mapping]\n",
    "\n",
    "# Reduce X by deleting the columns corresponding to important features\n",
    "reduced_X = np.delete(X, important_indices, axis=1)\n",
    "\n",
    "# Function 'evaluate_model' should be defined by you and should return some performance metric\n",
    "# Here, we mock this by assuming it returns accuracy\n",
    "reduced_performance = evaluate_model(reduced_X, y)\n",
    "print(\"Performance with reduced features:\", reduced_performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "balanced_acc_scores = []\n",
    "mcc_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "\n",
    "def create_simple_encoder():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "        GlobalAveragePooling1D()\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "max_sequence_length = max(len(seq) for seq in X_sequences)\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Define vocabulary size and embedding dimension\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e594c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_idx, (train_index, test_index) in enumerate(stratified_kfold_resampled.split(X_padded, y_encoded), 1):\n",
    "    X_train_fold, X_test_fold = X_padded[train_index], X_padded[test_index]\n",
    "    y_train_fold, y_test_fold = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "    # Create and compile the simple encoder model\n",
    "    simple_encoder = create_simple_encoder()\n",
    "    simple_encoder.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the simple encoder on the training data\n",
    "    simple_encoder.fit(X_train_fold, y_train_fold, epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "    # Get embeddings for the training and test data\n",
    "    X_train_embeddings_fold = simple_encoder.predict(X_train_fold)\n",
    "    X_test_embeddings_fold = simple_encoder.predict(X_test_fold)\n",
    "\n",
    "    # Resample the training data\n",
    "    X_train_resampled_fold, y_train_resampled_fold = smote_enn.fit_resample(X_train_embeddings_fold, y_train_fold)\n",
    "\n",
    "    # Perform grid search on the entire resampled training data\n",
    "    grid_search.fit(X_train_resampled_fold, y_train_resampled_fold)\n",
    "\n",
    "    # Get the best hyperparameters from grid search\n",
    "    best_params = grid_search.best_params_\n",
    "    best_logistic_classifier = grid_search.best_estimator_\n",
    "\n",
    "    # Train the best SVM model on the entire resampled training data\n",
    "    best_logistic_classifier.fit(X_train_resampled_fold, y_train_resampled_fold)\n",
    "\n",
    "    # Predict labels on the test fold\n",
    "    y_pred_fold = best_logistic_classifier.predict(X_test_embeddings_fold)\n",
    "\n",
    "    # Calculate evaluation metrics for this fold\n",
    "    balanced_acc_fold = balanced_accuracy_score(y_test_fold, y_pred_fold)\n",
    "    mcc_fold = matthews_corrcoef(y_test_fold, y_pred_fold)\n",
    "    f1_fold = f1_score(y_test_fold, y_pred_fold)\n",
    "    recall_fold = recall_score(y_test_fold, y_pred_fold,average='weighted')\n",
    "\n",
    "    # Append the scores to the lists\n",
    "    balanced_acc_scores.append(balanced_acc_fold)\n",
    "    mcc_scores.append(mcc_fold)\n",
    "    f1_scores.append(f1_fold)\n",
    "    recall_scores.append(recall_fold)\n",
    "\n",
    "    # Print the evaluation metrics for this fold\n",
    "    print(f\"Fold {fold_idx}:\")\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_acc_fold:.2f}\")\n",
    "    print(f\"F1-Score: {f1_fold:.2f}\")\n",
    "    print(f\"Matthew's Correlation Coefficient: {mcc_fold:.2f}\")\n",
    "    print(f\"Recall: {recall_fold:.2f}\")\n",
    "    print()\n",
    "\n",
    "# Print average scores\n",
    "print(\"Average Scores simple encoder:\")\n",
    "print(f\"{np.mean(balanced_acc_scores):.2f}\")\n",
    "print(f\"{np.mean(f1_scores):.2f}\")\n",
    "print(f\"{np.mean(mcc_scores):.2f}\")\n",
    "print(f\"{np.mean(recall_scores):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1874a9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
