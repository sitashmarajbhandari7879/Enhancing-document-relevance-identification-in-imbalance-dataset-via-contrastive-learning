{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from nltk import pos_tag\n",
    "\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from synergy_dataset import Dataset, iter_datasets\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b37173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(file_name):\n",
    "    d = Dataset(file_name)\n",
    "    data_dict = d.to_dict([\"authorships\", \"cited_by_count\", \"publication_year\"])\n",
    "    d.metadata\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for openalex_id, data in data_dict.items():\n",
    "        authorships = data.get(\"authorships\", [])\n",
    "        cited_by_count = data.get(\"cited_by_count\", None)\n",
    "        publication_year = data.get(\"publication_year\", None)\n",
    "\n",
    "        # Append the extracted data as a dictionary to the data_list\n",
    "        data_list.append({\n",
    "            \"openalex-id\": openalex_id,\n",
    "            \"authorships\": authorships,\n",
    "            \"cited_by_count\": cited_by_count,\n",
    "            \"publication_year\": publication_year\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the data_list\n",
    "    df_metadata = pd.DataFrame(data_list)\n",
    "\n",
    "    df_labels = d.to_frame()\n",
    "    df_labels.reset_index(inplace=True)\n",
    "    df_labels = df_labels.rename(columns={'openalex_id': 'openalex-id'})\n",
    "    df = pd.merge(df_metadata, df_labels, on=\"openalex-id\", how=\"left\")\n",
    "\n",
    "    author_names = df['authorships'].apply(lambda x: ', '.join([item['author']['display_name'] for item in x]))\n",
    "\n",
    "    df['author_names'] = author_names\n",
    "    columns_to_drop = ['authorships']\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9003e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text, for_embedding=False):\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)  # remove white space\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")  # removes tags\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)  # keep only ASCII and European characters\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "    if for_embedding:\n",
    "        # Keep punctuation\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", str(text))\n",
    "    text = re.sub(RE_ASCII, \" \", str(text))\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", str(text))\n",
    "    text = re.sub(RE_WSPACE, \" \", str(text))\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(word_tokens)\n",
    "\n",
    "    # Filter words to keep only nouns and adjectives\n",
    "    filtered_words = [word for word, tag in pos_tags if tag.startswith('NN') or tag.startswith('JJ')]\n",
    "    \n",
    "    if not for_embedding:\n",
    "        filtered_words = [word.lower() for word in filtered_words]  # Convert to lowercase\n",
    "        filtered_words = [lemmatizer.lemmatize(word) for word in filtered_words if word not in stop_words]  # Lemmatize and remove stopwords\n",
    "\n",
    "    text_clean = \" \".join(filtered_words)\n",
    "    return text_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a2d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"Hall_2012\" \n",
    "# file_name = \"Jeyaraman_2020\"\n",
    "# file_name = \"Radjenovic_2013\"\n",
    "# file_name = \"Smid_2020\"\n",
    "df = get_dataset(file_name)\n",
    "df['Corpus'] = df['title'] + ' ' + df['abstract']\n",
    "df['Corpus'] = df['Corpus'].apply(clean_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4907cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = file_name + \"_cleaned.csv\"\n",
    "df.to_csv(output_csv, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
