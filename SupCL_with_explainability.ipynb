{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel,RobertaConfig\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import precision_recall_fscore_support, matthews_corrcoef, balanced_accuracy_score\n",
    "\n",
    "\n",
    "from SupCL_Seq import SupCsTrainer\n",
    "import torch.nn as nn\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_path = 'Hall_2012_cleaned.csv'\n",
    "# file_path = 'Jeyaraman_2020_cleaned.csv'\n",
    "# file_path = 'Radjenovic_2013_cleaned.csv'\n",
    "# file_path = 'Smid_2020_cleaned.csv'\n",
    "\n",
    "df = pd.read_csv(file_path, delimiter=',')\n",
    "df = df.dropna(axis=0)\n",
    "num_df = df. shape[0]\n",
    "\n",
    "print(f\"Number of data: {num_df}\")\n",
    "class_counts_df = df['label_included'].value_counts()\n",
    "print(class_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary') # or 'micro', 'macro', 'weighted' based on your needs\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "    balanced_acc = balanced_accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'mcc': mcc,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ef160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet, words\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ')\n",
    "            synonyms.add(synonym)\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(sentence, n):\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word.isalpha()]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def random_insertion(sentence, n):\n",
    "    words = sentence.split()\n",
    "    for _ in range(n):\n",
    "        new_synonyms = []\n",
    "        random_word = random.choice(words)\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if synonyms:\n",
    "            new_synonym = random.choice(synonyms)\n",
    "            insert_position = random.randint(0, len(words))\n",
    "            words.insert(insert_position, new_synonym)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def random_deletion(sentence, p):\n",
    "    words = sentence.split()\n",
    "    if len(words) == 1:\n",
    "        return sentence\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "    if len(new_words) == 0:\n",
    "        return random.choice(words)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def augment_text(df, minority_class, augment_by):\n",
    "    minority_df = df[df['label_included'] == minority_class]\n",
    "    n_minority = len(minority_df)\n",
    "    n_augmentations = int(n_minority * augment_by)\n",
    "    augmented_texts = []\n",
    "    for _ in range(n_augmentations):\n",
    "        original_text = random.choice(minority_df['Corpus'].tolist())\n",
    "        augmented_text = original_text\n",
    "        # Choose a random augmentation technique\n",
    "        augmentation_type = random.choice(['synonym_replacement', 'random_insertion', 'random_deletion'])\n",
    "        if augmentation_type == 'synonym_replacement':\n",
    "            augmented_text = synonym_replacement(augmented_text, n=1)\n",
    "        elif augmentation_type == 'random_insertion':\n",
    "            augmented_text = random_insertion(augmented_text, n=1)\n",
    "        elif augmentation_type == 'random_deletion':\n",
    "            augmented_text = random_deletion(augmented_text, p=0.25)\n",
    "        augmented_texts.append(augmented_text)\n",
    "    augmented_df = pd.DataFrame(augmented_texts, columns=['Corpus'])\n",
    "    augmented_df['label_included'] = minority_class\n",
    "    return augmented_df\n",
    "\n",
    "# Example usage\n",
    "df_augmented = augment_text(df, minority_class=1, augment_by=0.6)\n",
    "df_sample = pd.concat([df, df_augmented], ignore_index=True)\n",
    "df_sample = df_sample.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "texts = df_sample['Corpus'].tolist()\n",
    "labels = df_sample['label_included'].tolist()\n",
    "class_counts = df_sample['label_included'].value_counts()\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7464abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_texts, train_y, test_labels = train_test_split(df_sample['Corpus'], df_sample['label_included'], test_size=0.2, stratify=df_sample['label_included'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1055f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= \"roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(test_texts), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f56c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, dataloader):\n",
    "    model.eval()\n",
    "    model.to('cuda')\n",
    "\n",
    "    embeddings = []\n",
    "    for batch in dataloader:\n",
    "        inputs = {key: val.to('cuda') for key, val in batch.items() if key != 'labels'}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Take the embeddings from the last hidden state for the [CLS] token\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(cls_embeddings)\n",
    "\n",
    "    # Convert list of embeddings into a single numpy array\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d48f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "\n",
    "class MetricsLogger(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.training_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.accuracy = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Logs might contain training loss, validation loss, and validation metrics\n",
    "        if 'loss' in logs:  # Training loss\n",
    "            self.training_loss.append(logs['loss'])\n",
    "        if 'eval_loss' in logs:  # Validation loss\n",
    "            self.validation_loss.append(logs['eval_loss'])\n",
    "        if 'eval_accuracy' in logs:  # Accuracy\n",
    "            self.accuracy.append(logs['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d41dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "pre_train_embeddings = extract_embeddings(model, train_dataloader)\n",
    "\n",
    "num_labels = len(set(labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels)  # Convert labels to a tensor for consistent indexing\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]  # Direct tensor indexing\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = CustomDataset(val_encodings, val_labels.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2817e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "pre_train_embeddings = extract_embeddings(model, train_dataloader)\n",
    "\n",
    "num_labels = len(set(labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc099fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_args = TrainingArguments(\n",
    "        output_dir = 'iterations_1/results',\n",
    "        save_total_limit = 1,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=10,  \n",
    "        evaluation_strategy = 'no',  # Changed to 'epoch' to ensure evaluation happens\n",
    "        logging_strategy='epoch', \n",
    "        eval_steps = 500,# Log metrics at the end of each epoch\n",
    "        save_strategy='epoch', \n",
    "        warmup_steps=50, \n",
    "        learning_rate = 1e-05,\n",
    "        report_to ='tensorboard',\n",
    "        weight_decay=0.01,               \n",
    "        logging_dir='./logs',\n",
    "    )\n",
    "metrics_logger = MetricsLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25814a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SupCL_trainer = SupCsTrainer.SupCsTrainer(\n",
    "            w_drop_out=[0.0,0.05],\n",
    "            temperature= 0.05,\n",
    "            def_drop_out=0.1,\n",
    "            pooling_strategy='mean',\n",
    "            model = model,\n",
    "            args = CL_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "#             callbacks=[metrics_logger],\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SupCL_trainer.train()\n",
    "SupCL_trainer.save_model('iterations_1/cs_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e7daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('iterations_1/cs_baseline', num_labels=num_labels)\n",
    "fine_tuned_base_model = model.roberta\n",
    "# Freeze the base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c1da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_train_embeddings = extract_embeddings(fine_tuned_base_model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir = './results_1',\n",
    "        save_total_limit = 1,\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=10,  \n",
    "        per_device_eval_batch_size=10,\n",
    "        evaluation_strategy = 'epoch',\n",
    "        eval_steps = 500,\n",
    "        learning_rate = 5e-03,\n",
    "        logging_strategy='epoch',  # Log metrics at the end of each epoch\n",
    "        save_strategy='epoch',\n",
    "        report_to ='tensorboard',\n",
    "        weight_decay=0.01,               \n",
    "        logging_dir='./logs',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a96d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "            model,\n",
    "            args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            callbacks=[metrics_logger],\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use t-SNE to reduce dimensionality for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_initial_embeddings = tsne.fit_transform(pre_train_embeddings)\n",
    "tsne_final_embeddings = tsne.fit_transform(post_train_embeddings)\n",
    "\n",
    "# Visualize the embeddings\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(tsne_initial_embeddings[:, 0], tsne_initial_embeddings[:, 1], c=train_labels, cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('Initial Embeddings')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(tsne_final_embeddings[:, 0], tsne_final_embeddings[:, 1], c=train_labels, cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('Final Embeddings After Training')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87069154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "pre_silhouette = silhouette_score(pre_train_embeddings, train_labels)\n",
    "post_silhouette = silhouette_score(post_train_embeddings, train_labels)\n",
    "\n",
    "print(f\"Silhouette score before training: {pre_silhouette}\")\n",
    "print(f\"Silhouette score after training: {post_silhouette}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d5db47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb52aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev_metrics = {\n",
    "    'precision': np.std(fold_metrics['precision']),\n",
    "    'recall': np.std(fold_metrics['recall']),\n",
    "    'f1': np.std(fold_metrics['f1']),\n",
    "    'mcc': np.std(fold_metrics['mcc']),\n",
    "    'balanced_accuracy': np.std(fold_metrics['balanced_accuracy'])\n",
    "}\n",
    "\n",
    "# Optionally, you can print these values to see them\n",
    "for metric, std_dev in std_dev_metrics.items():\n",
    "    print(f\"The standard deviation for {metric} is {std_dev:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_metrics = {metric: np.mean(values) for metric, values in fold_metrics.items()}\n",
    "\n",
    "# Print aggregate metrics\n",
    "print(\"Aggregate Metrics Across All Folds:\")\n",
    "for metric, value in aggregate_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Plotting confusion matrix with seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe134eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from lime.lime_text import IndexedString,TextDomainMapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa44313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(texts):\n",
    "    # Convert texts to the format the model expects\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    \n",
    "    # Move inputs to the correct device\n",
    "    inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Create a LIME Text Explainer\n",
    "lime_explainer = LimeTextExplainer(class_names=[\"Irrelevant\", \"Relevant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def f(x):\n",
    "    tv = torch.tensor(\n",
    "        [\n",
    "            tokenizer.encode(v, padding=\"max_length\", max_length=500, truncation=True)\n",
    "            for v in x\n",
    "        ]\n",
    "    ).cuda()\n",
    "    outputs = model(tv)[0].detach().cpu().numpy()\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    val = sp.special.logit(scores[:, 1])  # use one vs rest logit units\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae09fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for hall\n",
    "# texts = [\"towards logistic regression model fault prone code software project paper challenge logistic regression model able fault prone object class software project several study successful result design complexity metric purpose data exploration distribution metric varies project task project difficult first endeavor problem simple log transformation design complexity comparable project transformation project data spread data prediction model\",\n",
    "#        \"uml design metric fault prone class java system identifying software problem implementation cheaper implementation hence fault proneness software module early software artifact software design beneficial software engineer early prediction fault motivation consideration composition usefulness uml design metric fault proneness java historical data significant industrial java system uml prediction model case study level detail message import sequence diagram significant predictor class fault proneness prediction model uml design metric better accuracy built code metric\",\n",
    "       \n",
    "#        \"nonstationary motor fault detection using recent quadratic time frequency representation use electric motor aerospace transportation industry condition time fault detection electric motor importance motor diagnostics nonstationary environment difficult sophisticated signal processing technique recent time plethora new time frequency distribution analysis nonstationary signal superior frequency resolution zhao atlas mark distribution distribution paper use new time frequency distribution nonstationary fault diagnostics electric motor common myth quadratic time frequency distribution suitable commercial implementation paper issue detail optimal discrete time implementation quadratic time frequency distribution time frequency representation digital signal processing platform method\",\n",
    "#        \"determination bga structural defect joint defect ray laminography equipment software ray laminography recent year latest system feature small spl mu diameter location spl mu dimension ic package detail complex spaced structure microlaminographs resolution method microlaminography paper technology methodology result microlaminography system failure analysis ic packaging copper trace layer bga substrate bond wire short plane solder resist lot bga assembly subsequent verification destructive physical analysis dpa reconstruction individual solder ball assembly defect analysis normal ray dpa information\"]\n",
    "\n",
    "# # this is for Jeyaraman_2020_cleaned\n",
    "# texts = [\"clinical radiographical year long term outcome microfracture autologous chondrocyte implantation pair analysis purpose clinical radiographical long term outcome microfracture mfx first generation periosteum autologous chondrocyte implantation aci method subject knee joint aci microfracture post operative least year clinical pre post operative outcome numeric analog scale na pain lysholm tegner ikdc koos radiographical evaluation magnetic resonance mri assessment regenerate quality magnetic resonance observation cartilage repair tissue mocart knee osteoarthritis system mko relaxation time rt map microstructural cartilage analysis result mfx aci patient female male age year good long term outcome low pain score significant improved clinical score final lysholm functional na score higher mfx group lysholm mfx v aci na function mfx v aci mocart score qualitative difference kos analysis cartilage repair small defect significant better outcome relaxation time difference group region regenerate tissue conclusion study coherent statistical difference cartilage repair procedure mfx superior treatment small cartilage defect\",\n",
    "#         \"novel minimally invasive technique cartilage repair human knee using arthroscopic microfracture injection mesenchymal stem cell hyaluronic acid prospective comparative study safety short term efficacy introduction current cell cartilage repair technique form scaffold separate surgical procedure novel scaffold le technique cartilage repair human knee arthroscopic microfracture outpatient intra articular injection autologous bone marrow mesenchymal stem cell msc hyaluronic acid ha material method seventy age sex lesion size knee symptomatic cartilage defect underwent cartilage repair technique open technique msc beneath periosteal patch defect prospective evaluation group international cartilage repair society icrs cartilage injury evaluation package question short form sf health survey international knee documentation committee ikdc subjective knee evaluation form lysholm knee scale tegner activity level scale postoperative magnetic resonance mri evaluation year patient significant adverse event course study final follow mean month significant improvement mean ikdc lysholm sf physical component score visual analogue pain score treatment group conclusion short term result novel technique comparable open procedure added advantage invasive single operation general anaesthesia safety efficacy ongoing trial key word chondral novel osteoarthritis regeneration\",\n",
    "#         \"biological effect bone marrow concentrate knee pathology abstract population active incidence cost knee pain acute injury symptomatic knee osteoarthritis current treatment method short respect ability intra articular environment normal joint homeostasis basic science clinical evidence efficacy cell therapy bone marrow concentrate bmc promise nonsurgical joint treatment approach bmc inherent advantage treatment various knee pathology point care orthobiologic product delivers growth factor inflammatory protein mesenchymal stem cell evidence use bmc repair focal cartilage defect treatment generalized knee pain high quality study necessary clinical utility bmc particular attention patient selection aspiration processing reporting functional outcome\",\n",
    "#         \"arthroscopic technique fixation three dimensional scaffold autologous chondrocyte transplantation structural property vitro model aim present study structural property matrix autologous chondrocyte implantation multiple fixation technique fresh porcine knee undergone load failure ultimate failure load yield load stiffness different technique fixation mm thick polymer fleece fixation biodegradable polylevolactide transosseous technique conventional suture fixation technique pin transosseous anchoring maximum load yield load higher group fixation group transosseous group conventional suture stiffness higher group group biomechanical data fixation technique fixation transosseous higher ultimate load yield load stiffness conventional suture technique time point data bioceed biotissue technology gmbh freiburg germany biomechanical data outstanding fixation strength arthroscopic technique bioceed matrix scaffold autologous chondrocyte transplantation thus arthroscopic fixation biomaterial patient turn research arthroscopic technique biomaterial\"]\n",
    "# # #this is for rajv\n",
    "# texts = [\"prediction fault proneness early phase object development complexity object software several metric chidamber kemerer metric object metric effectiveness viewpoint fault proneness object software evaluation metric design specification source code inner complexity class information algorithm class structure end design phase estimation fault proneness early phase effort fault newspaper publisher new method fault proneness object class early phase several complexity metric object software method checkpoint analysis design implementation phase fault prone applicable metric checkpoint\",\n",
    "#         \"non invasive label free quantitative characterisation live cell monolayer culture paper development evanescent wave microscope predictive software quantitative study cellular process live cell quality high resolution use label technique current measurement capability live cell light microscopy critical data cell adhesion particular mouse neural stem cell tirm technique result tirm image high information content containing detail cell morphology cell adhesion combination time lapse provide information cell dynamic process motility\",\n",
    "#         \"knowledge system faulty component detection production testing electronic device knowledge system faulty component detection identification production testing analog electronic board main part guided measuring probe diagnostic expert system result inductive machine learning technique diagnostic rule acquisition\"]\n",
    "# 10,01,01\n",
    "texts = [\"empirical approach software fault prediction measuring software quality term fault proneness data tomorrow programmer fault prone area project development faulty area previous developed project experienced professional development fault prone module person faulty area solution minimum time budget turn increase software quality client satisfaction fuzzy mean clustering technique prediction faulty non faulty module project datasets training module available nasa project cm pc jm requirement code metric combination metric model model result combination metric model best prediction model approach others literature accurate approach matlab\",\n",
    "         \"conceptual coupling metric object oriented system coupling software maintainability metric predictor external software quality fault proneness impact analysis ripple effect change many measure object oo software specific dimension paper new set measure oo system conceptual coupling semantic information source code identifier comment case study open source software system new measure structural coupling case study conceptual coupling new dimension measure metric\",\n",
    "         \"fault prediction capability program file logical coupling metric frequent change source file bug software metric source file paper approach set metric logical coupling source file metric historical data software change fixing post release bug propose set metric number bug capable bug prediction model experiment experimental result propose set metric number bug hence bug prediction model experiment accuracy bug predictor model\"\n",
    "    ]\n",
    "# 01,10,10\n",
    "# texts =[\"static analysis tool early indicator pre release defect density software development helpful early estimate defect density software component estimate fault prone area code empirical approach early prediction pre release density defect static analysis defect different static analysis tool actual pre release defect density window server strong positive correlation static analysis defect density pre release defect density pre release defect density actual pre release defect density high degree statistical significance discriminant analysis result static analysis tool high low quality component overall classification rate\",\n",
    "#        \"fault cached history version history software system fault prone entity basic assumption fault isolation burst several related fault location likely fault location fault location location fault location location cache moment fault developer likely fault prone location useful verification validation resource fault prone file entity evaluation open source project revision cache selects source code file fault significant advance state art\",\n",
    "#        \"fault content class challenge fault prediction today prediction possible low cost possible needing little data possible language average developer paper fault method summary available metric result sampled class fault content entire system method large software system java class line code evaluation fault generalization method good fault prone cluster possible value representative class\"]\n",
    "# #this is for smid\n",
    "# texts =[\"data dependent prior mitigate small sample bias latent growth model mixed effect model mem latent growth model lgms interchangeable discipline specific nomenclature software implementation model interchangeable small sample size maximum likelihood estimation small sample bias mem lgms bayesian method dependent asymptotics issue choice factor covariance matrix prior distribution substantial influence small sample tutorial difference lgms mem data dependent prior established class method frequentist bayesian paradigm small sample bias prevalent lgm software additional programming bare minimum\",\n",
    "#        \"country comparative survey analysis bayesian linear perspective meuleman billiet simulation study question many country accurate multilevel sem estimation comparative study author sample country accurate estimation bayesian estimation method structural equation much lower sample current study simulation meuleman billiet bayesian estimation lowest number country multilevel sem main result simulation sample country sufficient accurate bayesian estimation multilevel sem practicable number country available large scale comparative survey\",\n",
    "#        \"structural equation interchangeable dyad structural equation sem straightforward fashion data interchangeable dyad dyad member author general strategy sem model estimation comparison fit assessment dyad level pairwise dyadic data application approach actor partner interdependence model confirmatory factor analysis latent growth curve analysis\",\n",
    "#        \"performance method test upper level mediation presence nonnormal data monte carlo study statistical performance standard robust multilevel mediation analysis method indirect effect cluster experimental design various departure normality performance method upper level mediation process indirect effect effect group treatment person level outcome person level mediator method bias parametric percentile bootstrap empirical test best overall performance method nonnormal score distribution elevated type error rate poorer confidence interval coverage condition preliminary finding new mediation analysis method robust test indirect effect\"]\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Explanation for Text {i+1}:\")\n",
    "    exp = lime_explainer.explain_instance(text, predict_proba, num_features=20, labels=[1],num_samples=50)\n",
    "    fig = exp.as_pyplot_figure()\n",
    "    fig.savefig('lime_explanation.png', bbox_inches='tight')  # Save as PNG\n",
    "    plt.show()\n",
    "    feature_names = exp.as_list()\n",
    "    df = pd.DataFrame(feature_names, columns=['Feature', 'Weight'])\n",
    "    print(df)\n",
    "    \n",
    "    exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ace78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = shap.Explainer(f, tokenizer)\n",
    "\n",
    "shap_values = shap_explainer(texts, fixed_context=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82987dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, text_shap_values in enumerate(shap_values):\n",
    "    print(f\"SHAP values for Text {i + 1}:\")\n",
    "    shap.plots.text(text_shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text_shap_values in enumerate(shap_values):\n",
    "    print(f\"SHAP values for Text {i + 1}:\")\n",
    "    # Display the bar plot for the current text's SHAP values\n",
    "    shap.plots.bar(text_shap_values, max_display=40)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
