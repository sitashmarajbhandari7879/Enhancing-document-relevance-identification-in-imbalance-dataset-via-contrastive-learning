{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from transformers import Trainer\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as FF\n",
    "import torch.nn as nn\n",
    "import typing as tp\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "class SupCsTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        w_drop_out: tp.Optional[tp.List[float]] = [0.0,0.05,0.2],\n",
    "        temperature: tp.Optional[float] = 0.05,\n",
    "        def_drop_out: tp.Optional[float]=0.1,\n",
    "        pooling_strategy: tp.Optional[str]='pooler',\n",
    "        **kwargs\n",
    "        ):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.w_drop_out = w_drop_out\n",
    "        self.temperature_s = temperature \n",
    "        self.def_drop_out = def_drop_out\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        if pooling_strategy == 'pooler':\n",
    "            print('# Employing pooler ([CLS]) output.')\n",
    "        else:\n",
    "            print('# Employing mean of the last hidden layer.')\n",
    "        \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model: nn,\n",
    "        inputs: tp.Dict,\n",
    "        return_outputs: tp.Optional[bool]=False,\n",
    "        )-> tp.Tuple[float, torch.Tensor]:\n",
    "\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        # ----- Default p = 0.1 ---------#\n",
    "        output = model(**inputs)\n",
    "        if self.pooling_strategy == 'pooler':\n",
    "            try:\n",
    "                logits = output.pooler_output.unsqueeze(1) \n",
    "            except:\n",
    "                logits = output.last_hidden_state.mean(dim=1, keepdim=True)\n",
    "        else:\n",
    "            logits = output.last_hidden_state.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        # ---- iteratively create dropouts -----#\n",
    "        for p_dpr in self.w_drop_out:\n",
    "            # -- Set models dropout --#\n",
    "            if p_dpr != self.def_drop_out:\n",
    "                model = self.set_dropout_mf(model, w=p_dpr)\n",
    "            # ---- concat logits ------#\n",
    "            if self.pooling_strategy == 'pooler':\n",
    "                # --------- If model does offer pooler output --------#\n",
    "                try:\n",
    "                    logits = torch.cat((logits, model(**inputs).pooler_output.unsqueeze(1)), 1)\n",
    "                except:\n",
    "                    logits = torch.cat((logits, model(**inputs).last_hidden_state.mean(dim=1, keepdim=True)), 1)\n",
    "            else:\n",
    "                logits = torch.cat((logits, model(**inputs).last_hidden_state.mean(dim=1, keepdim=True)), 1)\n",
    "            \n",
    "        # ---- L2 norm ---------#\n",
    "        logits = FF.normalize(logits, p=2, dim=2)\n",
    "        \n",
    "        #----- Set model back to dropout = 0.1 -----#\n",
    "        if p_dpr != self.def_drop_out: model = self.set_dropout_mf(model, w=0.1)\n",
    "        \n",
    "        \n",
    "        # SupContrast\n",
    "        loss_fn = SupConLoss(temperature=self.temperature_s) # temperature=0.1\n",
    "\n",
    "        loss = loss_fn(logits, labels) # added rounding for stsb\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def set_dropout_mf(\n",
    "        self, \n",
    "        model:nn, \n",
    "        w:tp.List[float]\n",
    "        ):\n",
    "        \"\"\"Alters the dropouts in the embeddings.\n",
    "        \"\"\"\n",
    "        # ------ set hidden dropout -------#\n",
    "        if hasattr(model, 'module'):\n",
    "            model.module.embeddings.dropout.p = w\n",
    "            for i in model.module.encoder.layer:\n",
    "                i.attention.self.dropout.p = w\n",
    "                i.attention.output.dropout.p = w\n",
    "                i.output.dropout.p = w        \n",
    "        else:\n",
    "            model.embeddings.dropout.p = w\n",
    "            for i in model.encoder.layer:\n",
    "                i.attention.self.dropout.p = w\n",
    "                i.attention.output.dropout.p = w\n",
    "                i.output.dropout.p = w\n",
    "            \n",
    "        return model\n",
    "\n",
    "    \n",
    "class SupConLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
