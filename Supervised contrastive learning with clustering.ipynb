{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e7454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from googletrans import Translator\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import Counter\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a587df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = 'Hall_2012_cleaned.csv'\n",
    "# file_path = 'hall_keywords.csv'\n",
    "file_path = 'Jeyaraman_2020_cleaned.csv'\n",
    "# file_path = 'Radjenovic_2013_cleaned.csv'\n",
    "# file_path = 'Smid_2020_cleaned.csv'\n",
    "\n",
    "df = pd.read_csv(file_path, delimiter=',')\n",
    "df = df.dropna(axis=0)\n",
    "df_sample = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ')\n",
    "            synonyms.add(synonym)\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(sentence, n):\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word.isalpha()]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n: # only replace up to n words\n",
    "            break\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence\n",
    "\n",
    "def augment_text(df, minority_class, augment_by=0.9):\n",
    "    minority_df = df[df['label_included'] == minority_class]\n",
    "    n_augmentations = int(len(minority_df) * augment_by)\n",
    "    \n",
    "    augmented_texts = []\n",
    "    for _ in range(n_augmentations):\n",
    "        original_text = random.choice(minority_df['Corpus'].tolist())\n",
    "        augmented_text = synonym_replacement(original_text, n=1) # You can adjust n for more replacements\n",
    "        augmented_texts.append(augmented_text)\n",
    "    \n",
    "    # Add augmented texts to the dataframe\n",
    "    augmented_df = pd.DataFrame(augmented_texts, columns=['Corpus'])\n",
    "    augmented_df['label_included'] = minority_class\n",
    "    return pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "# Assuming your minority class is identified, for example, as 1\n",
    "df_augmented = augment_text(df, minority_class=1, augment_by=0.9)\n",
    "\n",
    "\n",
    "df_sample = pd.concat([df, df_augmented], ignore_index=True)\n",
    "\n",
    "# Shuffle the dataframe to mix original and augmented examples (optional)\n",
    "df_sample = df_sample.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df_sample['label_included'].value_counts()\n",
    "label_1_count = label_counts.get(1, 0) \n",
    "label_0_count = label_counts.get(0, 0)\n",
    "print(label_1_count)\n",
    "print(label_0_count)\n",
    "max_sequence_length = max(len(text.split()) for text in df_sample['Corpus'])  \n",
    "num_classes = 1\n",
    "input_shape = (max_sequence_length,)\n",
    "from collections import Counter\n",
    "token_counts = Counter(word for sentence in  df_sample['Corpus'] for word in sentence.split())\n",
    "vocab_size = len(token_counts)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=200\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "hidden_units = 128\n",
    "projection_units = 128\n",
    "num_epochs = 5\n",
    "dropout_rate = 0.3\n",
    "temperature = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cc3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "def create_simple_encoder():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "        GlobalAveragePooling1D()\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(encoder, trainable=True):\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape, dtype=tf.int32)\n",
    "    features = encoder(inputs)\n",
    "\n",
    "    # Flatten the features (if needed) to prepare for the dense layer\n",
    "    features = keras.layers.Flatten()(features)\n",
    "\n",
    "    # Add a single dense layer for classification\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"simple-text-classifier\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            keras.metrics.BinaryAccuracy(),\n",
    "            balanced_accuracy_metric,\n",
    "            f1_score_metric,\n",
    "            mcc_metric,\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a57ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "#     def __init__(self, temperature=1, name=None):\n",
    "#         super().__init__(name=name)\n",
    "#         self.temperature = temperature\n",
    "\n",
    "#     def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "     \n",
    "#         feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "   \n",
    "#         logits = tf.divide(\n",
    "#             tf.matmul(\n",
    "#                 feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "#             ),\n",
    "#             self.temperature,\n",
    "#         )\n",
    "      \n",
    "#         return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
    "class SupervisedContrastiveLoss(keras.losses.Loss): #triplet\n",
    "    def __init__(self, margin=0.3, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.margin = margin\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Assuming labels are binary: 1 for similar pairs, 0 for dissimilar pairs\n",
    "        positive_mask = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "        negative_mask = ~positive_mask\n",
    "\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "        # Calculate pairwise cosine similarities\n",
    "        similarities = tf.matmul(\n",
    "            feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "        )\n",
    "\n",
    "        # Get positive and negative similarities\n",
    "        positive_similarity = tf.where(positive_mask, similarities, tf.zeros_like(similarities))\n",
    "        negative_similarity = tf.where(negative_mask, similarities, tf.zeros_like(similarities))\n",
    "\n",
    "        # Calculate triplet loss\n",
    "        loss = tf.maximum(negative_similarity - positive_similarity + self.margin, 0)\n",
    "\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def add_projection_head(encoder):\n",
    "    input_shape = (max_sequence_length,)  \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "   \n",
    "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"text-encoder_with_projection-head\"\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43819e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_with_projection_head = add_projection_head(encoder_model)\n",
    "encoder_with_projection_head.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=SupervisedContrastiveLoss(),\n",
    ")\n",
    "\n",
    "encoder_with_projection_head.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc_metric(y_true, y_pred):\n",
    "    true_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true * y_pred, 0, 1)))\n",
    "    true_negatives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    false_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    false_negatives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "    \n",
    "    denominator = tf.keras.backend.sqrt((true_positives + false_positives) * (true_positives + false_negatives) * (true_negatives + false_positives) * (true_negatives + false_negatives))\n",
    "    mcc = (true_positives * true_negatives - false_positives * false_negatives) / (denominator + tf.keras.backend.epsilon())\n",
    "    \n",
    "    return mcc\n",
    "\n",
    "\n",
    "def balanced_accuracy_metric(y_true, y_pred):\n",
    "    actual_positives = tf.math.reduce_sum(y_true)\n",
    "    actual_negatives = tf.math.reduce_sum(1 - y_true)\n",
    "    \n",
    "    epsilon = 1e-7  # Small constant to avoid division by zero\n",
    "\n",
    "    true_positives = tf.math.reduce_sum(y_true * tf.round(y_pred))\n",
    "    true_negatives = tf.math.reduce_sum((1 - y_true) * tf.round(1 - y_pred))\n",
    "    \n",
    "    balanced_accuracy = 0.5 * (true_positives / (actual_positives + epsilon) + true_negatives / (actual_negatives + epsilon))\n",
    "    \n",
    "    return balanced_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f1_score_metric(y_true, y_pred):\n",
    "    true_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_pred, 0, 1)))\n",
    "    actual_positives = tf.keras.backend.sum(y_true)\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
    "    recall = true_positives / (actual_positives + tf.keras.backend.epsilon())\n",
    "    \n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "    \n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Parameters\n",
    "n_splits = 5  # Number of folds\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_df['Corpus'])\n",
    "\n",
    "# Convert labels to numpy array for StratifiedKFold\n",
    "labels = train_df['label_included'].values\n",
    "\n",
    "for fold, (train_idx, validate_idx) in enumerate(kfold.split(train_df, labels)):\n",
    "    print(f\"Running Fold {fold + 1}/{n_splits}\")\n",
    "\n",
    "    # Split data\n",
    "    train_df_fold = train_df.iloc[train_idx]\n",
    "    validate_df_fold = train_df.iloc[validate_idx]\n",
    "\n",
    "    # Tokenize and pad sequences\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(train_df['Corpus'])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "    # Scale the data\n",
    "#     scaler = StandardScaler()\n",
    "#     padded_sequences = scaler.fit_transform(padded_sequences)\n",
    "   \n",
    "    extractor = tf.keras.Model(inputs=encoder_model.inputs,\n",
    "                           outputs=encoder_model.layers[-2].output)\n",
    "\n",
    "# Predict to get the embeddings\n",
    "    embeddings_before_training = extractor.predict(padded_sequences)\n",
    "\n",
    "    # Create and compile models\n",
    "    encoder_with_projection_head = add_projection_head(create_simple_encoder())\n",
    "    encoder_with_projection_head.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=SupervisedContrastiveLoss(),\n",
    "    )\n",
    "\n",
    "    # Train the encoder\n",
    "    history = encoder_with_projection_head.fit(\n",
    "        x=padded_sequences, \n",
    "        y=labels, \n",
    "        batch_size=batch_size, \n",
    "        epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    final_embeddings = encoder_model.predict(padded_sequences)\n",
    "    \n",
    "    # Prepare validation data\n",
    "    validate_sequences = tokenizer.texts_to_sequences(validate_df_fold['Corpus'])\n",
    "    validate_padded = pad_sequences(validate_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "#     validate_padded = scaler.transform(validate_padded)\n",
    "    validate_labels = validate_df_fold['label_included'].values.astype(int)\n",
    "\n",
    "    # Create and train classifier\n",
    "    classifier = create_classifier(encoder_with_projection_head, trainable=False)\n",
    "    history_classifier = classifier.fit(\n",
    "        x=padded_sequences, \n",
    "        y=labels, \n",
    "        batch_size=batch_size, \n",
    "        epochs=num_epochs,\n",
    "        validation_data=(validate_padded, validate_labels)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "# tokenizer.fit_on_texts(train_df['Corpus'])\n",
    "# train_sequences = tokenizer.texts_to_sequences(train_df['Corpus'])\n",
    "# train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "# scaler = StandardScaler()\n",
    "# train_padded = scaler.fit_transform(train_padded)\n",
    "\n",
    "# train_y = train_df['label_included'].values.astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e46239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, Normalize\n",
    "\n",
    "extractor = tf.keras.Model(inputs=encoder_model.inputs,\n",
    "                           outputs=encoder_model.layers[-2].output)\n",
    "\n",
    "# Predict to get the embeddings\n",
    "embeddings_before_training = extractor.predict(padded_sequences)\n",
    "   \n",
    "extractor_after_training = tf.keras.Model(inputs=encoder_with_projection_head.input, outputs=encoder_with_projection_head.layers[-1].output)  # use 'model.output' to get embeddings from the encoder part\n",
    "embeddings_after_training = extractor_after_training.predict(padded_sequences)\n",
    "\n",
    "\n",
    "# Flatten the embeddings for TSNE\n",
    "flatten_embeddings_before_training = embeddings_before_training.reshape(embeddings_before_training.shape[0], -1)\n",
    "flatten_embeddings_after_training = embeddings_after_training.reshape(embeddings_after_training.shape[0], -1)\n",
    "\n",
    "# Define a function to plot the embeddings\n",
    "def plot_embeddings(embeddings, labels, title, perplexity=30):\n",
    "    if embeddings.shape[0] < perplexity:\n",
    "        perplexity = embeddings.shape[0] - 1  # Set perplexity to one less than the number of samples\n",
    "        \n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=0)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Define colormap\n",
    "    unique_labels = np.unique(labels)\n",
    "    cmap = ListedColormap(plt.cm.get_cmap('viridis', len(unique_labels))(np.linspace(0, 1, len(unique_labels))))\n",
    "    norm = Normalize(vmin=min(unique_labels), vmax=max(unique_labels))\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap=cmap, norm=norm)\n",
    "    plt.colorbar(scatter, ticks=range(len(unique_labels)))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plotting the embeddings before and after training\n",
    "plot_embeddings(flatten_embeddings_before_training, labels, \"Embeddings Before Training\")\n",
    "plot_embeddings(flatten_embeddings_after_training, labels, \"Embeddings After Training\")\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "# Compute Silhouette Score for embeddings before training\n",
    "silhouette_before = silhouette_score(flatten_embeddings_before_training, labels)\n",
    "print(f\"Silhouette Score Before Training: {silhouette_before}\")\n",
    "\n",
    "# Compute Silhouette Score for embeddings after training\n",
    "silhouette_after = silhouette_score(flatten_embeddings_after_training, labels)\n",
    "print(f\"Silhouette Score After Training: {silhouette_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f46187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, balanced_accuracy_score, f1_score, roc_auc_score,confusion_matrix,average_precision_score,recall_score\n",
    "\n",
    "\n",
    "mcc = matthews_corrcoef(true_labels, predicted_labels)\n",
    "balanced_accuracy = balanced_accuracy_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "precision = average_precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "\n",
    "\n",
    "print(f'Matthews Correlation Coefficient: {mcc}')\n",
    "print(f'Balanced Accuracy: {balanced_accuracy}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'precision Score: {precision}')\n",
    "print(f'recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be30589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "confusion = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "\n",
    "# Reshape the confusion matrix to a 2x2 matrix\n",
    "confusion = confusion.reshape(2, 2)\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36604032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "def predict_proba(texts):\n",
    "    # Tokenize and pad the text sequences\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "#     padded = scaler.transform(padded)  # Assuming you want to scale as done in training\n",
    "    # Get predictions\n",
    "    pred = classifier.predict(padded)\n",
    "    # For binary classification, LIME expects probabilities for both classes\n",
    "    return np.hstack((1-pred, pred))\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=['Irrelevant', 'Relevant'])\n",
    "\n",
    "# Choose an instance to explain\n",
    "idx = 9\n",
    "# For example, explain prediction for the 10th document in your test set\n",
    "test_doc = test_df['Corpus'].iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(test_doc, predict_proba, num_features=10)\n",
    "\n",
    "# Show the explanation\n",
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure()\n",
    "plt.title('Feature contribution for classifying as Relevant')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1fb7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Predict the probabilities for the test set\n",
    "predicted_probs = predict_proba(test_df['Corpus'].tolist())\n",
    "\n",
    "# Step 2: Determine the predicted class based on probability threshold, e.g., > 0.5 for class 1\n",
    "predicted_classes = np.argmax(predicted_probs, axis=1)\n",
    "\n",
    "# Adding a column for predicted classes to the test_df for convenience\n",
    "test_df['predicted_class'] = predicted_classes\n",
    "\n",
    "# Filter instances that were predicted as class 1 ('Relevant')\n",
    "predicted_relevant_df = test_df[test_df['predicted_class'] == 1]\n",
    "\n",
    "\n",
    "for idx, row in predicted_relevant_df.iterrows():\n",
    "    test_doc = row['Corpus']\n",
    "    \n",
    "    # Generate explanation for this instance\n",
    "    exp = explainer.explain_instance(test_doc, predict_proba, num_features=10)\n",
    "    \n",
    "    print(f\"Explanation for document {idx} (Predicted as 'Relevant'):\")\n",
    "    exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8136ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering with the same encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab03989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import save_model\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering, OPTICS\n",
    "from sklearn.metrics import adjusted_rand_score, davies_bouldin_score, silhouette_score, normalized_mutual_info_score, jaccard_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = df_sample['Corpus']\n",
    "ground_truth_labels = df_sample['label_included']\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ed222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_pca_components(data, explained_variance_threshold=0.95):\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    num_components = np.argmax(explained_variance >= explained_variance_threshold) + 1\n",
    "    return num_components\n",
    "\n",
    "def perform_pca(data, num_components):\n",
    "    pca = PCA(n_components=num_components)\n",
    "    reduced_data = pca.fit_transform(data)\n",
    "    return reduced_data\n",
    "\n",
    "def kmeans_clustering(data, k):\n",
    "    # K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=k, init=\"k-means++\", n_init=50)\n",
    "    kmeans.fit(data)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    return cluster_labels\n",
    "\n",
    "def hierarchical_clustering(data, k):\n",
    "    cosine_similarity_matrix = cosine_similarity(data)\n",
    "    hierarchical_model = AgglomerativeClustering(n_clusters=k, affinity=\"euclidean\", linkage=\"ward\")\n",
    "    cluster_labels = hierarchical_model.fit_predict(cosine_similarity_matrix)\n",
    "    return cluster_labels\n",
    "\n",
    "def perform_dbscan(data, epsilon, min_samples):\n",
    "    dbscan_model = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "    cluster_labels = dbscan_model.fit_predict(data)\n",
    "    return cluster_labels\n",
    "\n",
    "def perform_spectral_clustering(data, n_clusters):\n",
    "    spectral_model = SpectralClustering(n_clusters=n_clusters)\n",
    "    cluster_labels = spectral_model.fit_predict(data)\n",
    "    return cluster_labels\n",
    "\n",
    "def perform_optics(data, min_samples, max_eps):\n",
    "    optics_model = OPTICS(min_samples=min_samples, max_eps=max_eps)\n",
    "    cluster_labels = optics_model.fit_predict(data)\n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "def evaluate_clustering(ground_truth_labels, cluster_labels, data):\n",
    "    '''ARI 1 is optimal\n",
    "    DBI, chscore lower the value better\n",
    "    silhouette,nmi,fmi homogenety, completness ad vmeaure close to 1\n",
    "    '''\n",
    "    ari = adjusted_rand_score(ground_truth_labels, cluster_labels)\n",
    "    dbi = davies_bouldin_score(data, cluster_labels)\n",
    "    silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "    nmi = normalized_mutual_info_score(ground_truth_labels, cluster_labels)\n",
    "\n",
    "    jaccard_coefficient = jaccard_score(ground_truth_labels, cluster_labels, average='micro')\n",
    "    \n",
    "    return ari, dbi, silhouette_avg, nmi, jaccard_coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.save('simple_encoder_model.h5')\n",
    "encoder_model = load_model('simple_encoder_model.h5')\n",
    "# Generate embeddings using the loaded encoder\n",
    "embeddings = encoder_model.predict(X_padded)\n",
    "embeddings_standardized = scaler.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ada52",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "for i in range(2, 15):\n",
    "    model = KMeans(n_clusters=i)\n",
    "    labels = model.fit_predict(embeddings_standardized)\n",
    "    silhouette_avg = silhouette_score(embeddings_standardized, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    \n",
    "optimal_k_sil = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "print(f\"Optimal number of clusters (k): {optimal_k_sil}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19948c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = find_optimal_pca_components(embeddings_standardized)\n",
    "\n",
    "# reduced_data_stand = perform_pca(embeddings_standardized, num_components)\n",
    "reduced_data_stand = perform_pca(embeddings_standardized, 2)\n",
    "kmeans_labels = kmeans_clustering(reduced_data_stand, 2)\n",
    "\n",
    "# kmeans_labels = kmeans_clustering(reduced_data_stand, optimal_k_sil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform your data using t-SNE\n",
    "combined_features_tsne  = tsne.fit_transform(reduced_data_stand)\n",
    "\n",
    "plt.scatter(combined_features_tsne[:, 0], combined_features_tsne[:, 1], c=kmeans_labels, cmap='rainbow')\n",
    "plt.title(\"K-Means Clustering with t-SNE Visualization\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf55d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "combined_features_umap = umap_model.fit_transform(reduced_data_stand)\n",
    "\n",
    "plt.scatter(combined_features_umap[:, 0], combined_features_umap[:, 1], c=kmeans_labels, cmap='rainbow')\n",
    "plt.title(\"K-Means Clustering with UMAP Visualization\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_k, dbi_k, silhouette_avg_k, nmi_k, js_k = evaluate_clustering(ground_truth_labels, kmeans_labels,reduced_data_stand)\n",
    "print(\"ARI:\", ari_k)\n",
    "print(\"Davies-Bouldin Index:\", dbi_k)\n",
    "print(\"Silhouette Score:\", silhouette_avg_k)\n",
    "print(\"NMI:\", nmi_k)\n",
    "print(\"Jaccard-coefficient:\", js_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28285dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dendrogram = sch.dendrogram(sch.linkage(reduced_data_stand, method='ward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d987627",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_linkage_distance = 450\n",
    "# Adjust this value as per your observation\n",
    "\n",
    "# Count the number of clusters based on the linkage distance\n",
    "num_clusters = sum(1 for d in dendrogram['dcoord'] if d[1] > optimal_linkage_distance)\n",
    "\n",
    "print(f\"Optimal number of clusters: {num_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a93de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal_k_hierarchical = num_clusters\n",
    "optimal_k_hierarchical = 2\n",
    "\n",
    "hierarchical_labels = hierarchical_clustering(reduced_data_stand, optimal_k_hierarchical)\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming 'hierarchical_labels' is your label array\n",
    "label_counts = Counter(hierarchical_labels)\n",
    "\n",
    "# Print the label counts\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: Count {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e1852",
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_h, dbi_h, silhouette_avg_h, nmi_h, js_h = evaluate_clustering(ground_truth_labels, hierarchical_labels, reduced_data_stand)\n",
    "print(\"ARI:\", ari_h)\n",
    "print(\"Davies-Bouldin Index:\", dbi_h)\n",
    "print(\"Silhouette Score:\", silhouette_avg_h)\n",
    "print(\"NMI:\", nmi_h)\n",
    "print(\"Jaccard-coefficient:\", js_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform your data using t-SNE\n",
    "combined_features_tsne  = tsne.fit_transform(reduced_data_stand)\n",
    "\n",
    "plt.scatter(combined_features_tsne[:, 0], combined_features_tsne[:, 1], c=hierarchical_labels, cmap='rainbow')\n",
    "plt.title(\"Hierarchical Clustering with t-SNE Visualization\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "combined_features_umap = umap_model.fit_transform(reduced_data_stand)\n",
    "\n",
    "plt.scatter(combined_features_umap[:, 0], combined_features_umap[:, 1], c=hierarchical_labels, cmap='rainbow')\n",
    "plt.title(\"Hierarchical Clustering with UMAP Visualization\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0140293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "neighbors = NearestNeighbors(n_neighbors=20)\n",
    "neighbors_fit = neighbors.fit(reduced_data_stand)\n",
    "distances, indices = neighbors_fit.kneighbors(reduced_data_stand)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(distances)\n",
    "num_features = reduced_data_stand.shape[1]\n",
    "print(\"Number of features:\", num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc5a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_dbscan = 3\n",
    "\n",
    "min_samples_dbscan = 5\n",
    "\n",
    "dbscan_labels = perform_dbscan(reduced_data_stand, epsilon_dbscan, min_samples_dbscan)\n",
    "ari_db, dbi_db, silhouette_avg_db, nmi_db, js_db = evaluate_clustering(ground_truth_labels, dbscan_labels, reduced_data_stand)\n",
    "print(\"ARI:\", ari_db)\n",
    "print(\"Davies-Bouldin Index:\", dbi_db)\n",
    "print(\"Silhouette Score:\", silhouette_avg_db)\n",
    "print(\"NMI:\", nmi_db)\n",
    "print(\"Jaccard-coefficient:\", js_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7cb52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform your data using t-SNE\n",
    "combined_features_tsne  = tsne.fit_transform(reduced_data_stand)\n",
    "\n",
    "plt.scatter(combined_features_tsne[:, 0], combined_features_tsne[:, 1], c=dbscan_labels, cmap='rainbow')\n",
    "plt.title(\"DBSCAN Clustering with t-SNE Visualization\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a28954",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "combined_features_umap = umap_model.fit_transform(reduced_data_stand)\n",
    "\n",
    "plt.scatter(combined_features_umap[:, 0], combined_features_umap[:, 1], c=dbscan_labels, cmap='rainbow')\n",
    "plt.title(\"DBSCAn Clustering with UMAP Visualization\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f462aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a range of possible cluster numbers\n",
    "cluster_range = range(2, 20)\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "\n",
    "\n",
    "# Using the Silhouette Score to find the optimal number of clusters\n",
    "for n_clusters in cluster_range:\n",
    "    spectral_model = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors')\n",
    "    spectral_labels = spectral_model.fit_predict(reduced_data_stand)\n",
    "    silhouette_avg = silhouette_score(reduced_data_stand, spectral_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    \n",
    "optimal_num_clusters = cluster_range[silhouette_scores.index(max(silhouette_scores))]\n",
    "print(f\"Optimal number of clusters: {optimal_num_clusters}\")\n",
    "\n",
    "# Plot the Silhouette Score\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(cluster_range, silhouette_scores, marker='o')\n",
    "plt.title(\"Silhouette Score for Spectral Clustering\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d4685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the clustering model \n",
    "spectral_model_rbf = SpectralClustering(n_clusters = 2, affinity = 'nearest_neighbors') \n",
    "# spectral_model_rbf = SpectralClustering(n_clusters = optimal_num_clusters, affinity = 'nearest_neighbors') \n",
    "  \n",
    "# Training the model and Storing the predicted cluster labels \n",
    "labels_rbf = spectral_model_rbf.fit_predict(reduced_data_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform your data using t-SNE\n",
    "combined_features_tsne  = tsne.fit_transform(reduced_data_stand)\n",
    "\n",
    "plt.scatter(combined_features_tsne[:, 0], combined_features_tsne[:, 1], c=labels_rbf, cmap='rainbow')\n",
    "plt.title(\"Spectral Clustering with t-SNE Visualization\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dbfbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "combined_features_umap = umap_model.fit_transform(reduced_data_stand)\n",
    "\n",
    "plt.scatter(combined_features_umap[:, 0], combined_features_umap[:, 1], c=labels_rbf, cmap='rainbow')\n",
    "plt.title(\"Spectral Clustering with UMAP Visualization\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df6461",
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_sc, dbi_sc, silhouette_avg_sc, nmi_sc, js_sc = evaluate_clustering(ground_truth_labels, labels_rbf, reduced_data_stand)\n",
    "print(\"ARI:\", ari_sc)\n",
    "print(\"Davies-Bouldin Index:\", dbi_sc)\n",
    "print(\"Silhouette Score:\", silhouette_avg_sc)\n",
    "print(\"NMI:\", nmi_sc)\n",
    "print(\"Jaccard-coefficient:\", js_sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7fbc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Corpus': X,\n",
    "    'ground_truth_labels': ground_truth_labels,\n",
    "    'kmeans_labels': kmeans_labels,\n",
    "    'hierarchical_labels': hierarchical_labels,\n",
    "    'dbscan_labels': dbscan_labels,\n",
    "    'spectral_labels': labels_rbf\n",
    "}\n",
    "\n",
    "df_result = pd.DataFrame(data)\n",
    "\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_misclassified_gt0_kmeans1 = df_result[(df_result['ground_truth_labels'] == 0) & (df_result['kmeans_labels'] == 1)]\n",
    "\n",
    "# Filter rows where ground truth is 1 but kmeans is 0\n",
    "df_misclassified_gt1_kmeans0 = df_result[(df_result['ground_truth_labels'] == 1) & (df_result['kmeans_labels'] == 0)]\n",
    "\n",
    "df_classified_gt0_kmeans0 = df_result[(df_result['ground_truth_labels'] == 0) & (df_result['kmeans_labels'] == 0)]\n",
    "\n",
    "# Filter rows where ground truth is 1 but kmeans is 0\n",
    "df_classified_gt1_kmeans1 = df_result[(df_result['ground_truth_labels'] == 1) & (df_result['kmeans_labels'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_misclassified_gt1_kmeans0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e046fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_misclassified_gt0_kmeans1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed29e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classified_gt0_kmeans0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17095baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classified_gt1_kmeans1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c70e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d03dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8143d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMF_modeling(df_data, components):\n",
    "    df = df_data.copy()\n",
    "\n",
    "    # Initialize TF-IDF vectorizer\n",
    "    model = TfidfVectorizer(stop_words='english', ngram_range=(2, 2))\n",
    "\n",
    "    # Fit and transform the TF-IDF matrix\n",
    "    word_vector = model.fit_transform(df['Corpus'])\n",
    "\n",
    "    # Fit NMF model\n",
    "    nmf_model = NMF(n_components=components, init='nndsvd')\n",
    "    nmf_model.fit(word_vector)\n",
    "\n",
    "    # Assign the dominant topic to each document\n",
    "    df['topic'] = nmf_model.transform(word_vector).argmax(axis=1)\n",
    "\n",
    "    topic_words_dict = {}\n",
    "    topic_words = {}\n",
    "\n",
    "    n_words = 10\n",
    "\n",
    "    feature_names = model.get_feature_names_out()\n",
    "\n",
    "    # Extract top words for each topic and calculate TF-IDF scores\n",
    "    for idx, topic in enumerate(nmf_model.components_):\n",
    "        top_words_idx = topic.argsort()[:-n_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "\n",
    "        # Get TF-IDF scores for the top words\n",
    "        tfidf_scores = [model.idf_[model.vocabulary_[word]] for word in top_words]\n",
    "\n",
    "        topic_words_dict[idx] = list(zip(top_words, tfidf_scores))\n",
    "        topic_words[idx] = top_words\n",
    "\n",
    "    # Add the topic_words_dict and topic_words to the DataFrame\n",
    "    df['topic_words_tfid'] = [topic_words_dict[i] for i in df['topic']]\n",
    "    df['topic_words'] = [topic_words[i] for i in df['topic']]\n",
    "\n",
    "    return df, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b4c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_missclassified_1_0, tfidf_model = NMF_modeling(df_misclassified_gt1_kmeans0, components=2)\n",
    "result_missclassified_0_1,tfidf_model_0_1 =NMF_modeling(df_misclassified_gt0_kmeans1, components=2)\n",
    "result_missclassified_0_0,tfidf_model_0_0 =NMF_modeling(df_classified_gt0_kmeans0, components=2)\n",
    "result_missclassified_1_1,tfidf_model_1_1 =NMF_modeling(df_classified_gt1_kmeans1, components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_missclassified_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_missclassified_1_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_missclassified_0_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c53aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_missclassified_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "def visualize_combined_topic_words(df):\n",
    "    # Concatenate all topic words and TF-IDF scores from the DataFrame\n",
    "    all_topic_words = [item for sublist in df['topic_words_tfid'] for item in sublist]\n",
    "    \n",
    "    # Create a WordCloud for combined topic words\n",
    "    combined_wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(all_topic_words))\n",
    "\n",
    "    # Plot the combined WordCloud\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(combined_wordcloud, interpolation='bilinear')\n",
    "    plt.title('Combined Topic Words')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Create a bar plot for combined topic words\n",
    "    combined_df = pd.DataFrame(all_topic_words, columns=['topic_words', 'tfidf_scores'])\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='topic_words', y='tfidf_scores', data=combined_df)\n",
    "    plt.title('Combined Topic Words')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize combined topic words for all rows\n",
    "visualize_combined_topic_words(result_missclassified_1_1)\n",
    "\n",
    "visualize_combined_topic_words(result_missclassified_0_0)\n",
    "visualize_combined_topic_words(result_missclassified_1_0)\n",
    "visualize_combined_topic_words(result_missclassified_0_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
